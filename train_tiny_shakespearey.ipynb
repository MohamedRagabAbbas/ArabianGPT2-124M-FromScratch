{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXIy2V4Js8XT",
        "outputId": "ae2aa2a0-8c18-47ed-adf5-d03edd90d49c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PsXTv7NlkWSt"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 3e-4\n",
        "num_epochs = 10\n",
        "top = 1000000\n",
        "dropout_rate: float = 0.1"
      ],
      "metadata": {
        "id": "JL_LjFHfkqy3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "j10O6ZHBkv2-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class GPT2Config:\n",
        "    block_size: int = 0\n",
        "    vocab_size: int = 0\n",
        "    n_embd: int = 0\n",
        "    n_layer: int = 0\n",
        "    n_head: int = 0"
      ],
      "metadata": {
        "id": "w_437veGkwN_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q,k,v,mask=None):\n",
        "    d_k = q.size(-1)\n",
        "    qk = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        qk = qk.permute(1, 0, 2, 3) + mask\n",
        "        qk = qk.permute(1, 0, 2, 3)\n",
        "    qk = F.softmax(qk, dim=-1)\n",
        "    new_qkv = torch.matmul(qk, v)\n",
        "    return new_qkv\n",
        "\n",
        "class Multihead_Self_Attention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Multihead_Self_Attention, self).__init__()\n",
        "        self.n_embd = config.n_embd\n",
        "        self.n_head = config.n_head\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.std_scaler = 1\n",
        "\n",
        "    def forward(self,x,mask=None):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        y = self.c_proj(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "YFmok2nSkybf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class mlp(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super(mlp,self).__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.activation = nn.GELU(approximate='tanh')\n",
        "        self.std_scaler = 1\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "B-DJ__fCkzau"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super(Block, self).__init__()\n",
        "        self.attn = Multihead_Self_Attention(config)\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = mlp(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.std_scaler = 1\n",
        "    def forward(self, x):\n",
        "\n",
        "        resdual_x = x\n",
        "        x = self.ln_1(x)\n",
        "        x = self.attn(x) + resdual_x\n",
        "\n",
        "        resdual_x = x\n",
        "        x = self.ln_2(x)\n",
        "        x = self.mlp(x) + resdual_x\n",
        "        return x"
      ],
      "metadata": {
        "id": "LdMhjT_ik0z_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class myGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(myGPT, self).__init__()\n",
        "        self.config = config\n",
        "        self.transformer = nn.ModuleDict({\n",
        "            'wte': nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            'wpe': nn.Embedding(config.block_size, config.n_embd),\n",
        "            'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            'ln_f': nn.LayerNorm(config.n_embd)\n",
        "        })\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        # weight sharing\n",
        "        self.transformer['wte'].weight = self.lm_head.weight\n",
        "        # apply weight initialization\n",
        "        self.apply(self.initionalization)\n",
        "\n",
        "    def initionalization(self,model):\n",
        "        std_linear = 0.02\n",
        "        std_embedding = 0.01\n",
        "        if hasattr(model, 'std_scaler'):\n",
        "            std_linear = (2 * self.config.n_layer) ** -0.5\n",
        "            std_embedding = (2 * self.config.n_layer) ** -0.5\n",
        "        if isinstance(model,nn.Linear):\n",
        "            nn.init.normal_(model.weight, mean = 0,std = std_linear)\n",
        "            if model.bias is not None:\n",
        "                nn.init.zeros_(model.bias)\n",
        "        elif isinstance(model,nn.Embedding):\n",
        "            nn.init.normal_(model.weight,mean=0,std=std_embedding) # following the offical openAI implementation\n",
        "\n",
        "    def forward(self, x,targets = None):\n",
        "        B, T = x.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=x.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(x) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPT2Config(**config_args)\n",
        "        model = myGPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "        return model"
      ],
      "metadata": {
        "id": "_gFYgv95k4MS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "    def __init__(self, batch_size, block_size):\n",
        "        self.batch_size = batch_size\n",
        "        self.block_size = block_size\n",
        "        self.pointer = 0\n",
        "        self.data = ''\n",
        "        with open('input.txt', 'r') as file:\n",
        "            self.data = file.read()\n",
        "        self.tokens = tiktoken.get_encoding('gpt2').encode(self.data)\n",
        "        self.n_batches = len(self.tokens) // (self.batch_size * self.block_size)\n",
        "\n",
        "    def next_batch(self):\n",
        "        start = self.pointer\n",
        "        end = start + self.batch_size * self.block_size\n",
        "        if end + 1 > len(self.tokens):\n",
        "            raise IndexError(\"End of data reached\")\n",
        "\n",
        "        mini_tokens = self.tokens[start:end + 1]\n",
        "        x = torch.tensor(mini_tokens[:-1], dtype=torch.long).view(self.batch_size, self.block_size)\n",
        "        y = torch.tensor(mini_tokens[1:], dtype=torch.long).view(self.batch_size, self.block_size)\n",
        "        self.pointer += self.batch_size * self.block_size\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "2CJBU-s4k7WT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "Block_size = 512\n",
        "Batch_size = 8\n",
        "config = GPT2Config(block_size=Block_size, vocab_size=50304, n_embd=768, n_layer=12, n_head=12)\n",
        "model = myGPT(config)\n",
        "model = model.to(get_device())\n",
        "model = torch.compile(model)\n",
        "x,y = DataLoader(batch_size=Batch_size, block_size=Block_size).next_batch()\n",
        "x,y = x.to(get_device()), y.to(get_device())\n",
        "logits,loss = model.forward(x,y)\n",
        "print(logits.size(), loss) # I am expecting to have a loss equal to the cross entropy loss which is -log(probability)\n",
        "                            # where each word follows uniform distription so the probability should be 1/vocb_size = 1/50257 = 0.0000199\n",
        "                            # so the loss should be -log(0.0000199) = 10.8"
      ],
      "metadata": {
        "id": "RVdDIpUKk8sU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "659fe062-c46d-4740-8867-d16f0d7a3484"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 512, 50304]) tensor(10.9048, device='cuda:0', grad_fn=<CompiledFunctionBackward>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_float32_matmul_precision('high')\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "num_epochs = 10\n",
        "for j in range(num_epochs):\n",
        "  dataloader = DataLoader(Batch_size,Block_size)\n",
        "  for i in range(dataloader.n_batches - 1):\n",
        "      optimizer.zero_grad()\n",
        "      x,y = dataloader.next_batch()\n",
        "      x,y = x.to(get_device()), y.to(get_device())\n",
        "      logits, loss = model(x, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      print(f\"epoch: {j+1}, iteratation {i}, loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "Bw8rydLCk-UN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b9fbdf2-20f8-4027-8fa7-576f916b3887"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1, iteratation 0, loss: 6.16850471496582\n",
            "epoch: 1, iteratation 1, loss: 6.320102214813232\n",
            "epoch: 1, iteratation 2, loss: 6.154117107391357\n",
            "epoch: 1, iteratation 3, loss: 6.107738971710205\n",
            "epoch: 1, iteratation 4, loss: 5.822941780090332\n",
            "epoch: 1, iteratation 5, loss: 5.622753143310547\n",
            "epoch: 1, iteratation 6, loss: 5.660984039306641\n",
            "epoch: 1, iteratation 7, loss: 5.650468826293945\n",
            "epoch: 1, iteratation 8, loss: 5.750833034515381\n",
            "epoch: 1, iteratation 9, loss: 5.6154937744140625\n",
            "epoch: 1, iteratation 10, loss: 5.91624641418457\n",
            "epoch: 1, iteratation 11, loss: 6.096148490905762\n",
            "epoch: 1, iteratation 12, loss: 6.335612773895264\n",
            "epoch: 1, iteratation 13, loss: 6.246296405792236\n",
            "epoch: 1, iteratation 14, loss: 6.016467094421387\n",
            "epoch: 1, iteratation 15, loss: 6.018744468688965\n",
            "epoch: 1, iteratation 16, loss: 5.859983444213867\n",
            "epoch: 1, iteratation 17, loss: 5.803045749664307\n",
            "epoch: 1, iteratation 18, loss: 6.014570236206055\n",
            "epoch: 1, iteratation 19, loss: 5.943429946899414\n",
            "epoch: 1, iteratation 20, loss: 6.134334564208984\n",
            "epoch: 1, iteratation 21, loss: 5.928828716278076\n",
            "epoch: 1, iteratation 22, loss: 6.075568199157715\n",
            "epoch: 1, iteratation 23, loss: 6.147333145141602\n",
            "epoch: 1, iteratation 24, loss: 6.183830261230469\n",
            "epoch: 1, iteratation 25, loss: 6.1314592361450195\n",
            "epoch: 1, iteratation 26, loss: 5.99854040145874\n",
            "epoch: 1, iteratation 27, loss: 6.018457412719727\n",
            "epoch: 1, iteratation 28, loss: 6.004734992980957\n",
            "epoch: 1, iteratation 29, loss: 5.8976335525512695\n",
            "epoch: 1, iteratation 30, loss: 5.750165939331055\n",
            "epoch: 1, iteratation 31, loss: 5.669554710388184\n",
            "epoch: 1, iteratation 32, loss: 5.824963569641113\n",
            "epoch: 1, iteratation 33, loss: 5.972021102905273\n",
            "epoch: 1, iteratation 34, loss: 5.9918365478515625\n",
            "epoch: 1, iteratation 35, loss: 5.953973293304443\n",
            "epoch: 1, iteratation 36, loss: 5.779131889343262\n",
            "epoch: 1, iteratation 37, loss: 5.946427345275879\n",
            "epoch: 1, iteratation 38, loss: 5.7561469078063965\n",
            "epoch: 1, iteratation 39, loss: 5.578352451324463\n",
            "epoch: 1, iteratation 40, loss: 5.708769798278809\n",
            "epoch: 1, iteratation 41, loss: 5.803950309753418\n",
            "epoch: 1, iteratation 42, loss: 5.638061046600342\n",
            "epoch: 1, iteratation 43, loss: 5.642689228057861\n",
            "epoch: 1, iteratation 44, loss: 5.87553071975708\n",
            "epoch: 1, iteratation 45, loss: 5.746302604675293\n",
            "epoch: 1, iteratation 46, loss: 5.617124557495117\n",
            "epoch: 1, iteratation 47, loss: 5.604647636413574\n",
            "epoch: 1, iteratation 48, loss: 5.6305952072143555\n",
            "epoch: 1, iteratation 49, loss: 5.524997711181641\n",
            "epoch: 1, iteratation 50, loss: 5.632206439971924\n",
            "epoch: 1, iteratation 51, loss: 5.555152893066406\n",
            "epoch: 1, iteratation 52, loss: 6.088578224182129\n",
            "epoch: 1, iteratation 53, loss: 6.017580032348633\n",
            "epoch: 1, iteratation 54, loss: 5.770600318908691\n",
            "epoch: 1, iteratation 55, loss: 5.912187576293945\n",
            "epoch: 1, iteratation 56, loss: 6.267666816711426\n",
            "epoch: 1, iteratation 57, loss: 6.118857383728027\n",
            "epoch: 1, iteratation 58, loss: 5.762968063354492\n",
            "epoch: 1, iteratation 59, loss: 5.896991729736328\n",
            "epoch: 1, iteratation 60, loss: 5.76984977722168\n",
            "epoch: 1, iteratation 61, loss: 5.726367473602295\n",
            "epoch: 1, iteratation 62, loss: 5.815226078033447\n",
            "epoch: 1, iteratation 63, loss: 5.738519668579102\n",
            "epoch: 1, iteratation 64, loss: 5.530228614807129\n",
            "epoch: 1, iteratation 65, loss: 5.733353614807129\n",
            "epoch: 1, iteratation 66, loss: 5.8472161293029785\n",
            "epoch: 1, iteratation 67, loss: 5.741300582885742\n",
            "epoch: 1, iteratation 68, loss: 5.686731338500977\n",
            "epoch: 1, iteratation 69, loss: 5.475281238555908\n",
            "epoch: 1, iteratation 70, loss: 5.375007152557373\n",
            "epoch: 1, iteratation 71, loss: 5.924930572509766\n",
            "epoch: 1, iteratation 72, loss: 5.823789596557617\n",
            "epoch: 1, iteratation 73, loss: 5.6120500564575195\n",
            "epoch: 1, iteratation 74, loss: 5.661484718322754\n",
            "epoch: 1, iteratation 75, loss: 5.827116012573242\n",
            "epoch: 1, iteratation 76, loss: 5.5974955558776855\n",
            "epoch: 1, iteratation 77, loss: 5.328438758850098\n",
            "epoch: 1, iteratation 78, loss: 5.352880001068115\n",
            "epoch: 1, iteratation 79, loss: 5.333106517791748\n",
            "epoch: 1, iteratation 80, loss: 6.093292236328125\n",
            "epoch: 2, iteratation 0, loss: 5.724525451660156\n",
            "epoch: 2, iteratation 1, loss: 5.658355712890625\n",
            "epoch: 2, iteratation 2, loss: 5.742171287536621\n",
            "epoch: 2, iteratation 3, loss: 5.73162317276001\n",
            "epoch: 2, iteratation 4, loss: 5.467442989349365\n",
            "epoch: 2, iteratation 5, loss: 5.300066947937012\n",
            "epoch: 2, iteratation 6, loss: 5.307197570800781\n",
            "epoch: 2, iteratation 7, loss: 5.26358699798584\n",
            "epoch: 2, iteratation 8, loss: 5.384819984436035\n",
            "epoch: 2, iteratation 9, loss: 5.261866569519043\n",
            "epoch: 2, iteratation 10, loss: 5.466187477111816\n",
            "epoch: 2, iteratation 11, loss: 5.54401969909668\n",
            "epoch: 2, iteratation 12, loss: 5.67926025390625\n",
            "epoch: 2, iteratation 13, loss: 5.637276649475098\n",
            "epoch: 2, iteratation 14, loss: 5.476637840270996\n",
            "epoch: 2, iteratation 15, loss: 5.484563827514648\n",
            "epoch: 2, iteratation 16, loss: 5.257629871368408\n",
            "epoch: 2, iteratation 17, loss: 5.22075080871582\n",
            "epoch: 2, iteratation 18, loss: 5.52640438079834\n",
            "epoch: 2, iteratation 19, loss: 5.368820667266846\n",
            "epoch: 2, iteratation 20, loss: 5.576633453369141\n",
            "epoch: 2, iteratation 21, loss: 5.3102264404296875\n",
            "epoch: 2, iteratation 22, loss: 5.598568916320801\n",
            "epoch: 2, iteratation 23, loss: 5.667103290557861\n",
            "epoch: 2, iteratation 24, loss: 5.740588188171387\n",
            "epoch: 2, iteratation 25, loss: 5.682405948638916\n",
            "epoch: 2, iteratation 26, loss: 5.538995265960693\n",
            "epoch: 2, iteratation 27, loss: 5.602688789367676\n",
            "epoch: 2, iteratation 28, loss: 5.595602035522461\n",
            "epoch: 2, iteratation 29, loss: 5.508135795593262\n",
            "epoch: 2, iteratation 30, loss: 5.364694595336914\n",
            "epoch: 2, iteratation 31, loss: 5.311576843261719\n",
            "epoch: 2, iteratation 32, loss: 5.425978660583496\n",
            "epoch: 2, iteratation 33, loss: 5.5491156578063965\n",
            "epoch: 2, iteratation 34, loss: 5.60579776763916\n",
            "epoch: 2, iteratation 35, loss: 5.556052207946777\n",
            "epoch: 2, iteratation 36, loss: 5.406710624694824\n",
            "epoch: 2, iteratation 37, loss: 5.5592498779296875\n",
            "epoch: 2, iteratation 38, loss: 5.3703508377075195\n",
            "epoch: 2, iteratation 39, loss: 5.188396453857422\n",
            "epoch: 2, iteratation 40, loss: 5.307851791381836\n",
            "epoch: 2, iteratation 41, loss: 5.470351696014404\n",
            "epoch: 2, iteratation 42, loss: 5.238880157470703\n",
            "epoch: 2, iteratation 43, loss: 5.261821746826172\n",
            "epoch: 2, iteratation 44, loss: 5.548188209533691\n",
            "epoch: 2, iteratation 45, loss: 5.416257381439209\n",
            "epoch: 2, iteratation 46, loss: 5.299050331115723\n",
            "epoch: 2, iteratation 47, loss: 5.203804016113281\n",
            "epoch: 2, iteratation 48, loss: 5.2892985343933105\n",
            "epoch: 2, iteratation 49, loss: 5.190564155578613\n",
            "epoch: 2, iteratation 50, loss: 5.30756950378418\n",
            "epoch: 2, iteratation 51, loss: 5.204607009887695\n",
            "epoch: 2, iteratation 52, loss: 5.647000312805176\n",
            "epoch: 2, iteratation 53, loss: 5.609429836273193\n",
            "epoch: 2, iteratation 54, loss: 5.414359092712402\n",
            "epoch: 2, iteratation 55, loss: 5.560670375823975\n",
            "epoch: 2, iteratation 56, loss: 5.881964683532715\n",
            "epoch: 2, iteratation 57, loss: 5.703582763671875\n",
            "epoch: 2, iteratation 58, loss: 5.361990451812744\n",
            "epoch: 2, iteratation 59, loss: 5.486916542053223\n",
            "epoch: 2, iteratation 60, loss: 5.4428911209106445\n",
            "epoch: 2, iteratation 61, loss: 5.382224082946777\n",
            "epoch: 2, iteratation 62, loss: 5.37933349609375\n",
            "epoch: 2, iteratation 63, loss: 5.277253150939941\n",
            "epoch: 2, iteratation 64, loss: 5.147772312164307\n",
            "epoch: 2, iteratation 65, loss: 5.337088584899902\n",
            "epoch: 2, iteratation 66, loss: 5.380763530731201\n",
            "epoch: 2, iteratation 67, loss: 5.310124397277832\n",
            "epoch: 2, iteratation 68, loss: 5.275915145874023\n",
            "epoch: 2, iteratation 69, loss: 5.088018417358398\n",
            "epoch: 2, iteratation 70, loss: 4.9866042137146\n",
            "epoch: 2, iteratation 71, loss: 5.604758262634277\n",
            "epoch: 2, iteratation 72, loss: 5.3768415451049805\n",
            "epoch: 2, iteratation 73, loss: 5.167325019836426\n",
            "epoch: 2, iteratation 74, loss: 5.217451095581055\n",
            "epoch: 2, iteratation 75, loss: 5.3875651359558105\n",
            "epoch: 2, iteratation 76, loss: 5.196532249450684\n",
            "epoch: 2, iteratation 77, loss: 4.9354681968688965\n",
            "epoch: 2, iteratation 78, loss: 4.9255781173706055\n",
            "epoch: 2, iteratation 79, loss: 4.9149370193481445\n",
            "epoch: 2, iteratation 80, loss: 5.737149238586426\n",
            "epoch: 3, iteratation 0, loss: 5.503914833068848\n",
            "epoch: 3, iteratation 1, loss: 5.465236663818359\n",
            "epoch: 3, iteratation 2, loss: 5.502684593200684\n",
            "epoch: 3, iteratation 3, loss: 5.503049373626709\n",
            "epoch: 3, iteratation 4, loss: 5.277939796447754\n",
            "epoch: 3, iteratation 5, loss: 5.0621337890625\n",
            "epoch: 3, iteratation 6, loss: 5.0991106033325195\n",
            "epoch: 3, iteratation 7, loss: 5.05272102355957\n",
            "epoch: 3, iteratation 8, loss: 5.1163249015808105\n",
            "epoch: 3, iteratation 9, loss: 5.029940605163574\n",
            "epoch: 3, iteratation 10, loss: 5.246867656707764\n",
            "epoch: 3, iteratation 11, loss: 5.296772003173828\n",
            "epoch: 3, iteratation 12, loss: 5.38926887512207\n",
            "epoch: 3, iteratation 13, loss: 5.333527565002441\n",
            "epoch: 3, iteratation 14, loss: 5.196735858917236\n",
            "epoch: 3, iteratation 15, loss: 5.198107719421387\n",
            "epoch: 3, iteratation 16, loss: 4.939126014709473\n",
            "epoch: 3, iteratation 17, loss: 4.9539079666137695\n",
            "epoch: 3, iteratation 18, loss: 5.259947776794434\n",
            "epoch: 3, iteratation 19, loss: 5.025393962860107\n",
            "epoch: 3, iteratation 20, loss: 5.272266387939453\n",
            "epoch: 3, iteratation 21, loss: 4.960225582122803\n",
            "epoch: 3, iteratation 22, loss: 5.327766418457031\n",
            "epoch: 3, iteratation 23, loss: 5.4011383056640625\n",
            "epoch: 3, iteratation 24, loss: 5.499026775360107\n",
            "epoch: 3, iteratation 25, loss: 5.443609237670898\n",
            "epoch: 3, iteratation 26, loss: 5.308211326599121\n",
            "epoch: 3, iteratation 27, loss: 5.37695837020874\n",
            "epoch: 3, iteratation 28, loss: 5.360141277313232\n",
            "epoch: 3, iteratation 29, loss: 5.281959533691406\n",
            "epoch: 3, iteratation 30, loss: 5.125419616699219\n",
            "epoch: 3, iteratation 31, loss: 5.059113025665283\n",
            "epoch: 3, iteratation 32, loss: 5.176268577575684\n",
            "epoch: 3, iteratation 33, loss: 5.333319664001465\n",
            "epoch: 3, iteratation 34, loss: 5.388035774230957\n",
            "epoch: 3, iteratation 35, loss: 5.3184285163879395\n",
            "epoch: 3, iteratation 36, loss: 5.166103363037109\n",
            "epoch: 3, iteratation 37, loss: 5.302587032318115\n",
            "epoch: 3, iteratation 38, loss: 5.139044761657715\n",
            "epoch: 3, iteratation 39, loss: 4.925252914428711\n",
            "epoch: 3, iteratation 40, loss: 5.073358535766602\n",
            "epoch: 3, iteratation 41, loss: 5.258185386657715\n",
            "epoch: 3, iteratation 42, loss: 5.022390365600586\n",
            "epoch: 3, iteratation 43, loss: 5.056102752685547\n",
            "epoch: 3, iteratation 44, loss: 5.376059532165527\n",
            "epoch: 3, iteratation 45, loss: 5.2381792068481445\n",
            "epoch: 3, iteratation 46, loss: 5.102289199829102\n",
            "epoch: 3, iteratation 47, loss: 5.002717018127441\n",
            "epoch: 3, iteratation 48, loss: 5.09254264831543\n",
            "epoch: 3, iteratation 49, loss: 4.995532989501953\n",
            "epoch: 3, iteratation 50, loss: 5.110720634460449\n",
            "epoch: 3, iteratation 51, loss: 4.9744768142700195\n",
            "epoch: 3, iteratation 52, loss: 5.450413703918457\n",
            "epoch: 3, iteratation 53, loss: 5.439023017883301\n",
            "epoch: 3, iteratation 54, loss: 5.24472188949585\n",
            "epoch: 3, iteratation 55, loss: 5.371572494506836\n",
            "epoch: 3, iteratation 56, loss: 5.704317092895508\n",
            "epoch: 3, iteratation 57, loss: 5.523731231689453\n",
            "epoch: 3, iteratation 58, loss: 5.185728549957275\n",
            "epoch: 3, iteratation 59, loss: 5.274565696716309\n",
            "epoch: 3, iteratation 60, loss: 5.276618957519531\n",
            "epoch: 3, iteratation 61, loss: 5.196455955505371\n",
            "epoch: 3, iteratation 62, loss: 5.1697998046875\n",
            "epoch: 3, iteratation 63, loss: 5.006660461425781\n",
            "epoch: 3, iteratation 64, loss: 4.916267395019531\n",
            "epoch: 3, iteratation 65, loss: 5.101797103881836\n",
            "epoch: 3, iteratation 66, loss: 5.137216567993164\n",
            "epoch: 3, iteratation 67, loss: 5.04313850402832\n",
            "epoch: 3, iteratation 68, loss: 5.017389297485352\n",
            "epoch: 3, iteratation 69, loss: 4.839688301086426\n",
            "epoch: 3, iteratation 70, loss: 4.726137161254883\n",
            "epoch: 3, iteratation 71, loss: 5.418275833129883\n",
            "epoch: 3, iteratation 72, loss: 5.146230697631836\n",
            "epoch: 3, iteratation 73, loss: 4.908930778503418\n",
            "epoch: 3, iteratation 74, loss: 4.94791316986084\n",
            "epoch: 3, iteratation 75, loss: 5.123898506164551\n",
            "epoch: 3, iteratation 76, loss: 4.972737789154053\n",
            "epoch: 3, iteratation 77, loss: 4.695673942565918\n",
            "epoch: 3, iteratation 78, loss: 4.682025909423828\n",
            "epoch: 3, iteratation 79, loss: 4.687826156616211\n",
            "epoch: 3, iteratation 80, loss: 5.538726806640625\n",
            "epoch: 4, iteratation 0, loss: 5.373476028442383\n",
            "epoch: 4, iteratation 1, loss: 5.334810733795166\n",
            "epoch: 4, iteratation 2, loss: 5.352614402770996\n",
            "epoch: 4, iteratation 3, loss: 5.363812446594238\n",
            "epoch: 4, iteratation 4, loss: 5.098672866821289\n",
            "epoch: 4, iteratation 5, loss: 4.8570356369018555\n",
            "epoch: 4, iteratation 6, loss: 4.9242987632751465\n",
            "epoch: 4, iteratation 7, loss: 4.87119722366333\n",
            "epoch: 4, iteratation 8, loss: 4.94575309753418\n",
            "epoch: 4, iteratation 9, loss: 4.886785984039307\n",
            "epoch: 4, iteratation 10, loss: 5.106644630432129\n",
            "epoch: 4, iteratation 11, loss: 5.1098785400390625\n",
            "epoch: 4, iteratation 12, loss: 5.119470596313477\n",
            "epoch: 4, iteratation 13, loss: 5.079618453979492\n",
            "epoch: 4, iteratation 14, loss: 4.992316246032715\n",
            "epoch: 4, iteratation 15, loss: 5.003837585449219\n",
            "epoch: 4, iteratation 16, loss: 4.692485809326172\n",
            "epoch: 4, iteratation 17, loss: 4.732443809509277\n",
            "epoch: 4, iteratation 18, loss: 5.051508903503418\n",
            "epoch: 4, iteratation 19, loss: 4.773624897003174\n",
            "epoch: 4, iteratation 20, loss: 5.05100154876709\n",
            "epoch: 4, iteratation 21, loss: 4.72959566116333\n",
            "epoch: 4, iteratation 22, loss: 5.137980937957764\n",
            "epoch: 4, iteratation 23, loss: 5.227147102355957\n",
            "epoch: 4, iteratation 24, loss: 5.315074443817139\n",
            "epoch: 4, iteratation 25, loss: 5.278104305267334\n",
            "epoch: 4, iteratation 26, loss: 5.124709129333496\n",
            "epoch: 4, iteratation 27, loss: 5.1773223876953125\n",
            "epoch: 4, iteratation 28, loss: 5.165618419647217\n",
            "epoch: 4, iteratation 29, loss: 5.081131935119629\n",
            "epoch: 4, iteratation 30, loss: 4.917768955230713\n",
            "epoch: 4, iteratation 31, loss: 4.820781707763672\n",
            "epoch: 4, iteratation 32, loss: 5.002235412597656\n",
            "epoch: 4, iteratation 33, loss: 5.163225173950195\n",
            "epoch: 4, iteratation 34, loss: 5.219614028930664\n",
            "epoch: 4, iteratation 35, loss: 5.145750522613525\n",
            "epoch: 4, iteratation 36, loss: 4.997992515563965\n",
            "epoch: 4, iteratation 37, loss: 5.136488914489746\n",
            "epoch: 4, iteratation 38, loss: 4.974996566772461\n",
            "epoch: 4, iteratation 39, loss: 4.761763572692871\n",
            "epoch: 4, iteratation 40, loss: 4.911986351013184\n",
            "epoch: 4, iteratation 41, loss: 5.094693183898926\n",
            "epoch: 4, iteratation 42, loss: 4.870295524597168\n",
            "epoch: 4, iteratation 43, loss: 4.916118144989014\n",
            "epoch: 4, iteratation 44, loss: 5.223663330078125\n",
            "epoch: 4, iteratation 45, loss: 5.078509330749512\n",
            "epoch: 4, iteratation 46, loss: 4.953771591186523\n",
            "epoch: 4, iteratation 47, loss: 4.80812931060791\n",
            "epoch: 4, iteratation 48, loss: 4.930024147033691\n",
            "epoch: 4, iteratation 49, loss: 4.8335089683532715\n",
            "epoch: 4, iteratation 50, loss: 4.957996368408203\n",
            "epoch: 4, iteratation 51, loss: 4.821686267852783\n",
            "epoch: 4, iteratation 52, loss: 5.315140724182129\n",
            "epoch: 4, iteratation 53, loss: 5.292286396026611\n",
            "epoch: 4, iteratation 54, loss: 5.100147247314453\n",
            "epoch: 4, iteratation 55, loss: 5.259456634521484\n",
            "epoch: 4, iteratation 56, loss: 5.562043190002441\n",
            "epoch: 4, iteratation 57, loss: 5.381041526794434\n",
            "epoch: 4, iteratation 58, loss: 5.062975883483887\n",
            "epoch: 4, iteratation 59, loss: 5.105088233947754\n",
            "epoch: 4, iteratation 60, loss: 5.124788284301758\n",
            "epoch: 4, iteratation 61, loss: 5.05346155166626\n",
            "epoch: 4, iteratation 62, loss: 5.012905597686768\n",
            "epoch: 4, iteratation 63, loss: 4.836149215698242\n",
            "epoch: 4, iteratation 64, loss: 4.752495288848877\n",
            "epoch: 4, iteratation 65, loss: 4.90892219543457\n",
            "epoch: 4, iteratation 66, loss: 4.924385070800781\n",
            "epoch: 4, iteratation 67, loss: 4.842979907989502\n",
            "epoch: 4, iteratation 68, loss: 4.828819274902344\n",
            "epoch: 4, iteratation 69, loss: 4.643675327301025\n",
            "epoch: 4, iteratation 70, loss: 4.54223108291626\n",
            "epoch: 4, iteratation 71, loss: 5.260044097900391\n",
            "epoch: 4, iteratation 72, loss: 4.979119777679443\n",
            "epoch: 4, iteratation 73, loss: 4.74118709564209\n",
            "epoch: 4, iteratation 74, loss: 4.771942138671875\n",
            "epoch: 4, iteratation 75, loss: 4.927471160888672\n",
            "epoch: 4, iteratation 76, loss: 4.797163963317871\n",
            "epoch: 4, iteratation 77, loss: 4.519086837768555\n",
            "epoch: 4, iteratation 78, loss: 4.489993095397949\n",
            "epoch: 4, iteratation 79, loss: 4.51967716217041\n",
            "epoch: 4, iteratation 80, loss: 5.3527374267578125\n",
            "epoch: 5, iteratation 0, loss: 5.2113752365112305\n",
            "epoch: 5, iteratation 1, loss: 5.164385795593262\n",
            "epoch: 5, iteratation 2, loss: 5.186058521270752\n",
            "epoch: 5, iteratation 3, loss: 5.175294399261475\n",
            "epoch: 5, iteratation 4, loss: 4.92759370803833\n",
            "epoch: 5, iteratation 5, loss: 4.673816204071045\n",
            "epoch: 5, iteratation 6, loss: 4.750583648681641\n",
            "epoch: 5, iteratation 7, loss: 4.710913181304932\n",
            "epoch: 5, iteratation 8, loss: 4.792425632476807\n",
            "epoch: 5, iteratation 9, loss: 4.749250411987305\n",
            "epoch: 5, iteratation 10, loss: 4.985683441162109\n",
            "epoch: 5, iteratation 11, loss: 4.957332134246826\n",
            "epoch: 5, iteratation 12, loss: 4.913531303405762\n",
            "epoch: 5, iteratation 13, loss: 4.868618488311768\n",
            "epoch: 5, iteratation 14, loss: 4.839667320251465\n",
            "epoch: 5, iteratation 15, loss: 4.841480255126953\n",
            "epoch: 5, iteratation 16, loss: 4.535170078277588\n",
            "epoch: 5, iteratation 17, loss: 4.61859130859375\n",
            "epoch: 5, iteratation 18, loss: 4.933045387268066\n",
            "epoch: 5, iteratation 19, loss: 4.601158618927002\n",
            "epoch: 5, iteratation 20, loss: 4.848711967468262\n",
            "epoch: 5, iteratation 21, loss: 4.541735649108887\n",
            "epoch: 5, iteratation 22, loss: 4.984622955322266\n",
            "epoch: 5, iteratation 23, loss: 5.06239128112793\n",
            "epoch: 5, iteratation 24, loss: 5.155014991760254\n",
            "epoch: 5, iteratation 25, loss: 5.126910209655762\n",
            "epoch: 5, iteratation 26, loss: 4.980892181396484\n",
            "epoch: 5, iteratation 27, loss: 5.010834693908691\n",
            "epoch: 5, iteratation 28, loss: 5.021402359008789\n",
            "epoch: 5, iteratation 29, loss: 4.947050094604492\n",
            "epoch: 5, iteratation 30, loss: 4.7761030197143555\n",
            "epoch: 5, iteratation 31, loss: 4.66457462310791\n",
            "epoch: 5, iteratation 32, loss: 4.865440368652344\n",
            "epoch: 5, iteratation 33, loss: 5.032083511352539\n",
            "epoch: 5, iteratation 34, loss: 5.092710494995117\n",
            "epoch: 5, iteratation 35, loss: 4.998452663421631\n",
            "epoch: 5, iteratation 36, loss: 4.853450298309326\n",
            "epoch: 5, iteratation 37, loss: 5.008964538574219\n",
            "epoch: 5, iteratation 38, loss: 4.859402656555176\n",
            "epoch: 5, iteratation 39, loss: 4.644658088684082\n",
            "epoch: 5, iteratation 40, loss: 4.786169052124023\n",
            "epoch: 5, iteratation 41, loss: 4.971054553985596\n",
            "epoch: 5, iteratation 42, loss: 4.767346382141113\n",
            "epoch: 5, iteratation 43, loss: 4.799785137176514\n",
            "epoch: 5, iteratation 44, loss: 5.109156608581543\n",
            "epoch: 5, iteratation 45, loss: 4.959253787994385\n",
            "epoch: 5, iteratation 46, loss: 4.853662014007568\n",
            "epoch: 5, iteratation 47, loss: 4.649388790130615\n",
            "epoch: 5, iteratation 48, loss: 4.803879737854004\n",
            "epoch: 5, iteratation 49, loss: 4.703603267669678\n",
            "epoch: 5, iteratation 50, loss: 4.821596622467041\n",
            "epoch: 5, iteratation 51, loss: 4.660355091094971\n",
            "epoch: 5, iteratation 52, loss: 5.180060386657715\n",
            "epoch: 5, iteratation 53, loss: 5.15219783782959\n",
            "epoch: 5, iteratation 54, loss: 4.9664506912231445\n",
            "epoch: 5, iteratation 55, loss: 5.133976459503174\n",
            "epoch: 5, iteratation 56, loss: 5.441607475280762\n",
            "epoch: 5, iteratation 57, loss: 5.267269134521484\n",
            "epoch: 5, iteratation 58, loss: 4.922586441040039\n",
            "epoch: 5, iteratation 59, loss: 4.991352081298828\n",
            "epoch: 5, iteratation 60, loss: 5.024774074554443\n",
            "epoch: 5, iteratation 61, loss: 4.918842792510986\n",
            "epoch: 5, iteratation 62, loss: 4.864898681640625\n",
            "epoch: 5, iteratation 63, loss: 4.722525119781494\n",
            "epoch: 5, iteratation 64, loss: 4.615603923797607\n",
            "epoch: 5, iteratation 65, loss: 4.759545803070068\n",
            "epoch: 5, iteratation 66, loss: 4.773646354675293\n",
            "epoch: 5, iteratation 67, loss: 4.70433235168457\n",
            "epoch: 5, iteratation 68, loss: 4.695540904998779\n",
            "epoch: 5, iteratation 69, loss: 4.497279167175293\n",
            "epoch: 5, iteratation 70, loss: 4.41926383972168\n",
            "epoch: 5, iteratation 71, loss: 5.170175552368164\n",
            "epoch: 5, iteratation 72, loss: 4.834414958953857\n",
            "epoch: 5, iteratation 73, loss: 4.598263263702393\n",
            "epoch: 5, iteratation 74, loss: 4.64354133605957\n",
            "epoch: 5, iteratation 75, loss: 4.791479587554932\n",
            "epoch: 5, iteratation 76, loss: 4.715466499328613\n",
            "epoch: 5, iteratation 77, loss: 4.400090217590332\n",
            "epoch: 5, iteratation 78, loss: 4.352952480316162\n",
            "epoch: 5, iteratation 79, loss: 4.397543430328369\n",
            "epoch: 5, iteratation 80, loss: 5.2536516189575195\n",
            "epoch: 6, iteratation 0, loss: 5.095189571380615\n",
            "epoch: 6, iteratation 1, loss: 5.01082181930542\n",
            "epoch: 6, iteratation 2, loss: 5.033865928649902\n",
            "epoch: 6, iteratation 3, loss: 5.002601623535156\n",
            "epoch: 6, iteratation 4, loss: 4.774064064025879\n",
            "epoch: 6, iteratation 5, loss: 4.493039131164551\n",
            "epoch: 6, iteratation 6, loss: 4.5755615234375\n",
            "epoch: 6, iteratation 7, loss: 4.530204772949219\n",
            "epoch: 6, iteratation 8, loss: 4.6443257331848145\n",
            "epoch: 6, iteratation 9, loss: 4.642880439758301\n",
            "epoch: 6, iteratation 10, loss: 4.858010292053223\n",
            "epoch: 6, iteratation 11, loss: 4.823671340942383\n",
            "epoch: 6, iteratation 12, loss: 4.760331153869629\n",
            "epoch: 6, iteratation 13, loss: 4.712245464324951\n",
            "epoch: 6, iteratation 14, loss: 4.716360569000244\n",
            "epoch: 6, iteratation 15, loss: 4.72137451171875\n",
            "epoch: 6, iteratation 16, loss: 4.409515380859375\n",
            "epoch: 6, iteratation 17, loss: 4.480677127838135\n",
            "epoch: 6, iteratation 18, loss: 4.811066150665283\n",
            "epoch: 6, iteratation 19, loss: 4.450858116149902\n",
            "epoch: 6, iteratation 20, loss: 4.690166473388672\n",
            "epoch: 6, iteratation 21, loss: 4.340859889984131\n",
            "epoch: 6, iteratation 22, loss: 4.8486552238464355\n",
            "epoch: 6, iteratation 23, loss: 4.928775310516357\n",
            "epoch: 6, iteratation 24, loss: 5.004515647888184\n",
            "epoch: 6, iteratation 25, loss: 4.958021640777588\n",
            "epoch: 6, iteratation 26, loss: 4.852392673492432\n",
            "epoch: 6, iteratation 27, loss: 4.893650054931641\n",
            "epoch: 6, iteratation 28, loss: 4.8910417556762695\n",
            "epoch: 6, iteratation 29, loss: 4.823029041290283\n",
            "epoch: 6, iteratation 30, loss: 4.64292049407959\n",
            "epoch: 6, iteratation 31, loss: 4.539687633514404\n",
            "epoch: 6, iteratation 32, loss: 4.753012180328369\n",
            "epoch: 6, iteratation 33, loss: 4.933538436889648\n",
            "epoch: 6, iteratation 34, loss: 4.971614837646484\n",
            "epoch: 6, iteratation 35, loss: 4.877804756164551\n",
            "epoch: 6, iteratation 36, loss: 4.742600917816162\n",
            "epoch: 6, iteratation 37, loss: 4.915874481201172\n",
            "epoch: 6, iteratation 38, loss: 4.727038383483887\n",
            "epoch: 6, iteratation 39, loss: 4.555514335632324\n",
            "epoch: 6, iteratation 40, loss: 4.721401691436768\n",
            "epoch: 6, iteratation 41, loss: 4.886281967163086\n",
            "epoch: 6, iteratation 42, loss: 4.665412425994873\n",
            "epoch: 6, iteratation 43, loss: 4.707642555236816\n",
            "epoch: 6, iteratation 44, loss: 5.010166168212891\n",
            "epoch: 6, iteratation 45, loss: 4.8607869148254395\n",
            "epoch: 6, iteratation 46, loss: 4.755954742431641\n",
            "epoch: 6, iteratation 47, loss: 4.550057411193848\n",
            "epoch: 6, iteratation 48, loss: 4.703226089477539\n",
            "epoch: 6, iteratation 49, loss: 4.61497163772583\n",
            "epoch: 6, iteratation 50, loss: 4.704525947570801\n",
            "epoch: 6, iteratation 51, loss: 4.546600341796875\n",
            "epoch: 6, iteratation 52, loss: 5.085481643676758\n",
            "epoch: 6, iteratation 53, loss: 5.041920185089111\n",
            "epoch: 6, iteratation 54, loss: 4.85459566116333\n",
            "epoch: 6, iteratation 55, loss: 5.0353240966796875\n",
            "epoch: 6, iteratation 56, loss: 5.350094318389893\n",
            "epoch: 6, iteratation 57, loss: 5.154143333435059\n",
            "epoch: 6, iteratation 58, loss: 4.8201446533203125\n",
            "epoch: 6, iteratation 59, loss: 4.8788042068481445\n",
            "epoch: 6, iteratation 60, loss: 4.932814598083496\n",
            "epoch: 6, iteratation 61, loss: 4.829383850097656\n",
            "epoch: 6, iteratation 62, loss: 4.743492126464844\n",
            "epoch: 6, iteratation 63, loss: 4.600101947784424\n",
            "epoch: 6, iteratation 64, loss: 4.532258987426758\n",
            "epoch: 6, iteratation 65, loss: 4.710449695587158\n",
            "epoch: 6, iteratation 66, loss: 4.663710117340088\n",
            "epoch: 6, iteratation 67, loss: 4.574101448059082\n",
            "epoch: 6, iteratation 68, loss: 4.601090431213379\n",
            "epoch: 6, iteratation 69, loss: 4.419154167175293\n",
            "epoch: 6, iteratation 70, loss: 4.3478522300720215\n",
            "epoch: 6, iteratation 71, loss: 5.068950653076172\n",
            "epoch: 6, iteratation 72, loss: 4.729984283447266\n",
            "epoch: 6, iteratation 73, loss: 4.470365047454834\n",
            "epoch: 6, iteratation 74, loss: 4.494776248931885\n",
            "epoch: 6, iteratation 75, loss: 4.671724319458008\n",
            "epoch: 6, iteratation 76, loss: 4.585554122924805\n",
            "epoch: 6, iteratation 77, loss: 4.292092800140381\n",
            "epoch: 6, iteratation 78, loss: 4.2430100440979\n",
            "epoch: 6, iteratation 79, loss: 4.289927959442139\n",
            "epoch: 6, iteratation 80, loss: 5.163987636566162\n",
            "epoch: 7, iteratation 0, loss: 4.972474098205566\n",
            "epoch: 7, iteratation 1, loss: 4.873373985290527\n",
            "epoch: 7, iteratation 2, loss: 4.907179832458496\n",
            "epoch: 7, iteratation 3, loss: 4.8940839767456055\n",
            "epoch: 7, iteratation 4, loss: 4.647356986999512\n",
            "epoch: 7, iteratation 5, loss: 4.369231700897217\n",
            "epoch: 7, iteratation 6, loss: 4.477097511291504\n",
            "epoch: 7, iteratation 7, loss: 4.426648139953613\n",
            "epoch: 7, iteratation 8, loss: 4.526294708251953\n",
            "epoch: 7, iteratation 9, loss: 4.551731109619141\n",
            "epoch: 7, iteratation 10, loss: 4.7429399490356445\n",
            "epoch: 7, iteratation 11, loss: 4.690126895904541\n",
            "epoch: 7, iteratation 12, loss: 4.625063896179199\n",
            "epoch: 7, iteratation 13, loss: 4.578343391418457\n",
            "epoch: 7, iteratation 14, loss: 4.584657669067383\n",
            "epoch: 7, iteratation 15, loss: 4.589818954467773\n",
            "epoch: 7, iteratation 16, loss: 4.281536102294922\n",
            "epoch: 7, iteratation 17, loss: 4.358960151672363\n",
            "epoch: 7, iteratation 18, loss: 4.731935501098633\n",
            "epoch: 7, iteratation 19, loss: 4.312496662139893\n",
            "epoch: 7, iteratation 20, loss: 4.568610191345215\n",
            "epoch: 7, iteratation 21, loss: 4.199671745300293\n",
            "epoch: 7, iteratation 22, loss: 4.751752853393555\n",
            "epoch: 7, iteratation 23, loss: 4.830638885498047\n",
            "epoch: 7, iteratation 24, loss: 4.9159464836120605\n",
            "epoch: 7, iteratation 25, loss: 4.881659507751465\n",
            "epoch: 7, iteratation 26, loss: 4.761517524719238\n",
            "epoch: 7, iteratation 27, loss: 4.80106258392334\n",
            "epoch: 7, iteratation 28, loss: 4.799569129943848\n",
            "epoch: 7, iteratation 29, loss: 4.755858898162842\n",
            "epoch: 7, iteratation 30, loss: 4.565225601196289\n",
            "epoch: 7, iteratation 31, loss: 4.448227882385254\n",
            "epoch: 7, iteratation 32, loss: 4.670634746551514\n",
            "epoch: 7, iteratation 33, loss: 4.877538204193115\n",
            "epoch: 7, iteratation 34, loss: 4.892516613006592\n",
            "epoch: 7, iteratation 35, loss: 4.771356582641602\n",
            "epoch: 7, iteratation 36, loss: 4.662631034851074\n",
            "epoch: 7, iteratation 37, loss: 4.865560531616211\n",
            "epoch: 7, iteratation 38, loss: 4.695723056793213\n",
            "epoch: 7, iteratation 39, loss: 4.478730201721191\n",
            "epoch: 7, iteratation 40, loss: 4.634324073791504\n",
            "epoch: 7, iteratation 41, loss: 4.862703323364258\n",
            "epoch: 7, iteratation 42, loss: 4.617427825927734\n",
            "epoch: 7, iteratation 43, loss: 4.656421661376953\n",
            "epoch: 7, iteratation 44, loss: 4.937302589416504\n",
            "epoch: 7, iteratation 45, loss: 4.760939598083496\n",
            "epoch: 7, iteratation 46, loss: 4.668398857116699\n",
            "epoch: 7, iteratation 47, loss: 4.452474594116211\n",
            "epoch: 7, iteratation 48, loss: 4.640779495239258\n",
            "epoch: 7, iteratation 49, loss: 4.544833660125732\n",
            "epoch: 7, iteratation 50, loss: 4.639743804931641\n",
            "epoch: 7, iteratation 51, loss: 4.467966556549072\n",
            "epoch: 7, iteratation 52, loss: 5.032977104187012\n",
            "epoch: 7, iteratation 53, loss: 4.9964118003845215\n",
            "epoch: 7, iteratation 54, loss: 4.8212690353393555\n",
            "epoch: 7, iteratation 55, loss: 4.974348068237305\n",
            "epoch: 7, iteratation 56, loss: 5.237936019897461\n",
            "epoch: 7, iteratation 57, loss: 5.049809455871582\n",
            "epoch: 7, iteratation 58, loss: 4.744710922241211\n",
            "epoch: 7, iteratation 59, loss: 4.801279067993164\n",
            "epoch: 7, iteratation 60, loss: 4.871565341949463\n",
            "epoch: 7, iteratation 61, loss: 4.749299049377441\n",
            "epoch: 7, iteratation 62, loss: 4.647908687591553\n",
            "epoch: 7, iteratation 63, loss: 4.48162317276001\n",
            "epoch: 7, iteratation 64, loss: 4.409665107727051\n",
            "epoch: 7, iteratation 65, loss: 4.607763767242432\n",
            "epoch: 7, iteratation 66, loss: 4.591054916381836\n",
            "epoch: 7, iteratation 67, loss: 4.534648418426514\n",
            "epoch: 7, iteratation 68, loss: 4.528793811798096\n",
            "epoch: 7, iteratation 69, loss: 4.330729961395264\n",
            "epoch: 7, iteratation 70, loss: 4.260410308837891\n",
            "epoch: 7, iteratation 71, loss: 4.975768089294434\n",
            "epoch: 7, iteratation 72, loss: 4.638406276702881\n",
            "epoch: 7, iteratation 73, loss: 4.388503551483154\n",
            "epoch: 7, iteratation 74, loss: 4.411676406860352\n",
            "epoch: 7, iteratation 75, loss: 4.574915409088135\n",
            "epoch: 7, iteratation 76, loss: 4.509185791015625\n",
            "epoch: 7, iteratation 77, loss: 4.215523719787598\n",
            "epoch: 7, iteratation 78, loss: 4.14450740814209\n",
            "epoch: 7, iteratation 79, loss: 4.228878974914551\n",
            "epoch: 7, iteratation 80, loss: 5.073273181915283\n",
            "epoch: 8, iteratation 0, loss: 4.865478038787842\n",
            "epoch: 8, iteratation 1, loss: 4.760457515716553\n",
            "epoch: 8, iteratation 2, loss: 4.817553520202637\n",
            "epoch: 8, iteratation 3, loss: 4.782834053039551\n",
            "epoch: 8, iteratation 4, loss: 4.581620216369629\n",
            "epoch: 8, iteratation 5, loss: 4.270030975341797\n",
            "epoch: 8, iteratation 6, loss: 4.395420551300049\n",
            "epoch: 8, iteratation 7, loss: 4.350382328033447\n",
            "epoch: 8, iteratation 8, loss: 4.445939540863037\n",
            "epoch: 8, iteratation 9, loss: 4.484879970550537\n",
            "epoch: 8, iteratation 10, loss: 4.68278169631958\n",
            "epoch: 8, iteratation 11, loss: 4.616415977478027\n",
            "epoch: 8, iteratation 12, loss: 4.4975786209106445\n",
            "epoch: 8, iteratation 13, loss: 4.492177963256836\n",
            "epoch: 8, iteratation 14, loss: 4.5036468505859375\n",
            "epoch: 8, iteratation 15, loss: 4.543344497680664\n",
            "epoch: 8, iteratation 16, loss: 4.227163314819336\n",
            "epoch: 8, iteratation 17, loss: 4.2960920333862305\n",
            "epoch: 8, iteratation 18, loss: 4.64630126953125\n",
            "epoch: 8, iteratation 19, loss: 4.256057262420654\n",
            "epoch: 8, iteratation 20, loss: 4.529271125793457\n",
            "epoch: 8, iteratation 21, loss: 4.132460594177246\n",
            "epoch: 8, iteratation 22, loss: 4.668678283691406\n",
            "epoch: 8, iteratation 23, loss: 4.729707717895508\n",
            "epoch: 8, iteratation 24, loss: 4.833433151245117\n",
            "epoch: 8, iteratation 25, loss: 4.811331272125244\n",
            "epoch: 8, iteratation 26, loss: 4.695186138153076\n",
            "epoch: 8, iteratation 27, loss: 4.697204113006592\n",
            "epoch: 8, iteratation 28, loss: 4.6966352462768555\n",
            "epoch: 8, iteratation 29, loss: 4.676213264465332\n",
            "epoch: 8, iteratation 30, loss: 4.500783443450928\n",
            "epoch: 8, iteratation 31, loss: 4.36405086517334\n",
            "epoch: 8, iteratation 32, loss: 4.627366542816162\n",
            "epoch: 8, iteratation 33, loss: 4.792973041534424\n",
            "epoch: 8, iteratation 34, loss: 4.826372146606445\n",
            "epoch: 8, iteratation 35, loss: 4.7242841720581055\n",
            "epoch: 8, iteratation 36, loss: 4.585208892822266\n",
            "epoch: 8, iteratation 37, loss: 4.76145601272583\n",
            "epoch: 8, iteratation 38, loss: 4.608077049255371\n",
            "epoch: 8, iteratation 39, loss: 4.393655300140381\n",
            "epoch: 8, iteratation 40, loss: 4.555857181549072\n",
            "epoch: 8, iteratation 41, loss: 4.769328594207764\n",
            "epoch: 8, iteratation 42, loss: 4.526342391967773\n",
            "epoch: 8, iteratation 43, loss: 4.56821346282959\n",
            "epoch: 8, iteratation 44, loss: 4.8745622634887695\n",
            "epoch: 8, iteratation 45, loss: 4.7048659324646\n",
            "epoch: 8, iteratation 46, loss: 4.607644081115723\n",
            "epoch: 8, iteratation 47, loss: 4.3593573570251465\n",
            "epoch: 8, iteratation 48, loss: 4.535340309143066\n",
            "epoch: 8, iteratation 49, loss: 4.447495460510254\n",
            "epoch: 8, iteratation 50, loss: 4.559543609619141\n",
            "epoch: 8, iteratation 51, loss: 4.405138969421387\n",
            "epoch: 8, iteratation 52, loss: 4.941416263580322\n",
            "epoch: 8, iteratation 53, loss: 4.893041610717773\n",
            "epoch: 8, iteratation 54, loss: 4.726794719696045\n",
            "epoch: 8, iteratation 55, loss: 4.893543243408203\n",
            "epoch: 8, iteratation 56, loss: 5.1681904792785645\n",
            "epoch: 8, iteratation 57, loss: 4.9683685302734375\n",
            "epoch: 8, iteratation 58, loss: 4.6609649658203125\n",
            "epoch: 8, iteratation 59, loss: 4.696378231048584\n",
            "epoch: 8, iteratation 60, loss: 4.787903785705566\n",
            "epoch: 8, iteratation 61, loss: 4.67729377746582\n",
            "epoch: 8, iteratation 62, loss: 4.588153839111328\n",
            "epoch: 8, iteratation 63, loss: 4.444847106933594\n",
            "epoch: 8, iteratation 64, loss: 4.348038673400879\n",
            "epoch: 8, iteratation 65, loss: 4.528582572937012\n",
            "epoch: 8, iteratation 66, loss: 4.531615257263184\n",
            "epoch: 8, iteratation 67, loss: 4.471704959869385\n",
            "epoch: 8, iteratation 68, loss: 4.461118698120117\n",
            "epoch: 8, iteratation 69, loss: 4.260248184204102\n",
            "epoch: 8, iteratation 70, loss: 4.208896636962891\n",
            "epoch: 8, iteratation 71, loss: 4.911624908447266\n",
            "epoch: 8, iteratation 72, loss: 4.556557655334473\n",
            "epoch: 8, iteratation 73, loss: 4.308019638061523\n",
            "epoch: 8, iteratation 74, loss: 4.311807632446289\n",
            "epoch: 8, iteratation 75, loss: 4.473545074462891\n",
            "epoch: 8, iteratation 76, loss: 4.414567947387695\n",
            "epoch: 8, iteratation 77, loss: 4.124722003936768\n",
            "epoch: 8, iteratation 78, loss: 4.059849739074707\n",
            "epoch: 8, iteratation 79, loss: 4.150391578674316\n",
            "epoch: 8, iteratation 80, loss: 4.966517448425293\n",
            "epoch: 9, iteratation 0, loss: 4.761043548583984\n",
            "epoch: 9, iteratation 1, loss: 4.655017375946045\n",
            "epoch: 9, iteratation 2, loss: 4.7507123947143555\n",
            "epoch: 9, iteratation 3, loss: 4.705107688903809\n",
            "epoch: 9, iteratation 4, loss: 4.515128135681152\n",
            "epoch: 9, iteratation 5, loss: 4.19939661026001\n",
            "epoch: 9, iteratation 6, loss: 4.325305461883545\n",
            "epoch: 9, iteratation 7, loss: 4.262399673461914\n",
            "epoch: 9, iteratation 8, loss: 4.375151634216309\n",
            "epoch: 9, iteratation 9, loss: 4.412435054779053\n",
            "epoch: 9, iteratation 10, loss: 4.62278938293457\n",
            "epoch: 9, iteratation 11, loss: 4.55369758605957\n",
            "epoch: 9, iteratation 12, loss: 4.41457986831665\n",
            "epoch: 9, iteratation 13, loss: 4.444393157958984\n",
            "epoch: 9, iteratation 14, loss: 4.43578577041626\n",
            "epoch: 9, iteratation 15, loss: 4.495969295501709\n",
            "epoch: 9, iteratation 16, loss: 4.176992416381836\n",
            "epoch: 9, iteratation 17, loss: 4.223910331726074\n",
            "epoch: 9, iteratation 18, loss: 4.593555927276611\n",
            "epoch: 9, iteratation 19, loss: 4.189214706420898\n",
            "epoch: 9, iteratation 20, loss: 4.449685096740723\n",
            "epoch: 9, iteratation 21, loss: 4.061365604400635\n",
            "epoch: 9, iteratation 22, loss: 4.616471767425537\n",
            "epoch: 9, iteratation 23, loss: 4.672460556030273\n",
            "epoch: 9, iteratation 24, loss: 4.754037380218506\n",
            "epoch: 9, iteratation 25, loss: 4.70097017288208\n",
            "epoch: 9, iteratation 26, loss: 4.591013431549072\n",
            "epoch: 9, iteratation 27, loss: 4.660304069519043\n",
            "epoch: 9, iteratation 28, loss: 4.654379844665527\n",
            "epoch: 9, iteratation 29, loss: 4.596879959106445\n",
            "epoch: 9, iteratation 30, loss: 4.391849040985107\n",
            "epoch: 9, iteratation 31, loss: 4.295477867126465\n",
            "epoch: 9, iteratation 32, loss: 4.568271636962891\n",
            "epoch: 9, iteratation 33, loss: 4.741146087646484\n",
            "epoch: 9, iteratation 34, loss: 4.779453277587891\n",
            "epoch: 9, iteratation 35, loss: 4.645647048950195\n",
            "epoch: 9, iteratation 36, loss: 4.516833305358887\n",
            "epoch: 9, iteratation 37, loss: 4.670036792755127\n",
            "epoch: 9, iteratation 38, loss: 4.5274128913879395\n",
            "epoch: 9, iteratation 39, loss: 4.330745697021484\n",
            "epoch: 9, iteratation 40, loss: 4.465497016906738\n",
            "epoch: 9, iteratation 41, loss: 4.710453510284424\n",
            "epoch: 9, iteratation 42, loss: 4.463920593261719\n",
            "epoch: 9, iteratation 43, loss: 4.524588108062744\n",
            "epoch: 9, iteratation 44, loss: 4.805655479431152\n",
            "epoch: 9, iteratation 45, loss: 4.612330913543701\n",
            "epoch: 9, iteratation 46, loss: 4.5391459465026855\n",
            "epoch: 9, iteratation 47, loss: 4.262463569641113\n",
            "epoch: 9, iteratation 48, loss: 4.475903034210205\n",
            "epoch: 9, iteratation 49, loss: 4.393553733825684\n",
            "epoch: 9, iteratation 50, loss: 4.50142765045166\n",
            "epoch: 9, iteratation 51, loss: 4.355935096740723\n",
            "epoch: 9, iteratation 52, loss: 4.876870155334473\n",
            "epoch: 9, iteratation 53, loss: 4.826081275939941\n",
            "epoch: 9, iteratation 54, loss: 4.675581932067871\n",
            "epoch: 9, iteratation 55, loss: 4.841578483581543\n",
            "epoch: 9, iteratation 56, loss: 5.146609306335449\n",
            "epoch: 9, iteratation 57, loss: 4.9349870681762695\n",
            "epoch: 9, iteratation 58, loss: 4.630967617034912\n",
            "epoch: 9, iteratation 59, loss: 4.659884452819824\n",
            "epoch: 9, iteratation 60, loss: 4.74909782409668\n",
            "epoch: 9, iteratation 61, loss: 4.632185935974121\n",
            "epoch: 9, iteratation 62, loss: 4.529130935668945\n",
            "epoch: 9, iteratation 63, loss: 4.390937805175781\n",
            "epoch: 9, iteratation 64, loss: 4.32637882232666\n",
            "epoch: 9, iteratation 65, loss: 4.4611077308654785\n",
            "epoch: 9, iteratation 66, loss: 4.431613922119141\n",
            "epoch: 9, iteratation 67, loss: 4.35804557800293\n",
            "epoch: 9, iteratation 68, loss: 4.3641815185546875\n",
            "epoch: 9, iteratation 69, loss: 4.180910110473633\n",
            "epoch: 9, iteratation 70, loss: 4.136484622955322\n",
            "epoch: 9, iteratation 71, loss: 4.848320960998535\n",
            "epoch: 9, iteratation 72, loss: 4.473345756530762\n",
            "epoch: 9, iteratation 73, loss: 4.228589057922363\n",
            "epoch: 9, iteratation 74, loss: 4.243681907653809\n",
            "epoch: 9, iteratation 75, loss: 4.386698246002197\n",
            "epoch: 9, iteratation 76, loss: 4.347066879272461\n",
            "epoch: 9, iteratation 77, loss: 4.074643135070801\n",
            "epoch: 9, iteratation 78, loss: 4.003660678863525\n",
            "epoch: 9, iteratation 79, loss: 4.08951997756958\n",
            "epoch: 9, iteratation 80, loss: 4.9108500480651855\n",
            "epoch: 10, iteratation 0, loss: 4.69044303894043\n",
            "epoch: 10, iteratation 1, loss: 4.585306167602539\n",
            "epoch: 10, iteratation 2, loss: 4.6634345054626465\n",
            "epoch: 10, iteratation 3, loss: 4.60421895980835\n",
            "epoch: 10, iteratation 4, loss: 4.438106536865234\n",
            "epoch: 10, iteratation 5, loss: 4.144708156585693\n",
            "epoch: 10, iteratation 6, loss: 4.280592918395996\n",
            "epoch: 10, iteratation 7, loss: 4.208932876586914\n",
            "epoch: 10, iteratation 8, loss: 4.270973205566406\n",
            "epoch: 10, iteratation 9, loss: 4.345209121704102\n",
            "epoch: 10, iteratation 10, loss: 4.561403274536133\n",
            "epoch: 10, iteratation 11, loss: 4.50777006149292\n",
            "epoch: 10, iteratation 12, loss: 4.336864471435547\n",
            "epoch: 10, iteratation 13, loss: 4.334117889404297\n",
            "epoch: 10, iteratation 14, loss: 4.341560363769531\n",
            "epoch: 10, iteratation 15, loss: 4.399689674377441\n",
            "epoch: 10, iteratation 16, loss: 4.068950653076172\n",
            "epoch: 10, iteratation 17, loss: 4.16676139831543\n",
            "epoch: 10, iteratation 18, loss: 4.549347877502441\n",
            "epoch: 10, iteratation 19, loss: 4.117589950561523\n",
            "epoch: 10, iteratation 20, loss: 4.382190704345703\n",
            "epoch: 10, iteratation 21, loss: 3.9824278354644775\n",
            "epoch: 10, iteratation 22, loss: 4.522101879119873\n",
            "epoch: 10, iteratation 23, loss: 4.581857204437256\n",
            "epoch: 10, iteratation 24, loss: 4.663730144500732\n",
            "epoch: 10, iteratation 25, loss: 4.641446113586426\n",
            "epoch: 10, iteratation 26, loss: 4.4884443283081055\n",
            "epoch: 10, iteratation 27, loss: 4.564955234527588\n",
            "epoch: 10, iteratation 28, loss: 4.5514326095581055\n",
            "epoch: 10, iteratation 29, loss: 4.520952224731445\n",
            "epoch: 10, iteratation 30, loss: 4.343319416046143\n",
            "epoch: 10, iteratation 31, loss: 4.2420220375061035\n",
            "epoch: 10, iteratation 32, loss: 4.483013153076172\n",
            "epoch: 10, iteratation 33, loss: 4.670535087585449\n",
            "epoch: 10, iteratation 34, loss: 4.6924052238464355\n",
            "epoch: 10, iteratation 35, loss: 4.58607292175293\n",
            "epoch: 10, iteratation 36, loss: 4.457917213439941\n",
            "epoch: 10, iteratation 37, loss: 4.615495681762695\n",
            "epoch: 10, iteratation 38, loss: 4.455349922180176\n",
            "epoch: 10, iteratation 39, loss: 4.259086608886719\n",
            "epoch: 10, iteratation 40, loss: 4.396183490753174\n",
            "epoch: 10, iteratation 41, loss: 4.626039981842041\n",
            "epoch: 10, iteratation 42, loss: 4.382209777832031\n",
            "epoch: 10, iteratation 43, loss: 4.442162036895752\n",
            "epoch: 10, iteratation 44, loss: 4.760778427124023\n",
            "epoch: 10, iteratation 45, loss: 4.548713684082031\n",
            "epoch: 10, iteratation 46, loss: 4.479684829711914\n",
            "epoch: 10, iteratation 47, loss: 4.180569648742676\n",
            "epoch: 10, iteratation 48, loss: 4.400758743286133\n",
            "epoch: 10, iteratation 49, loss: 4.313056945800781\n",
            "epoch: 10, iteratation 50, loss: 4.436371803283691\n",
            "epoch: 10, iteratation 51, loss: 4.294729709625244\n",
            "epoch: 10, iteratation 52, loss: 4.796714782714844\n",
            "epoch: 10, iteratation 53, loss: 4.742480278015137\n",
            "epoch: 10, iteratation 54, loss: 4.626210689544678\n",
            "epoch: 10, iteratation 55, loss: 4.76312255859375\n",
            "epoch: 10, iteratation 56, loss: 5.054720878601074\n",
            "epoch: 10, iteratation 57, loss: 4.833569526672363\n",
            "epoch: 10, iteratation 58, loss: 4.5415143966674805\n",
            "epoch: 10, iteratation 59, loss: 4.615835189819336\n",
            "epoch: 10, iteratation 60, loss: 4.699807643890381\n",
            "epoch: 10, iteratation 61, loss: 4.5606889724731445\n",
            "epoch: 10, iteratation 62, loss: 4.433770179748535\n",
            "epoch: 10, iteratation 63, loss: 4.30167293548584\n",
            "epoch: 10, iteratation 64, loss: 4.247917175292969\n",
            "epoch: 10, iteratation 65, loss: 4.375104904174805\n",
            "epoch: 10, iteratation 66, loss: 4.386700630187988\n",
            "epoch: 10, iteratation 67, loss: 4.331147193908691\n",
            "epoch: 10, iteratation 68, loss: 4.334836006164551\n",
            "epoch: 10, iteratation 69, loss: 4.129637718200684\n",
            "epoch: 10, iteratation 70, loss: 4.069263458251953\n",
            "epoch: 10, iteratation 71, loss: 4.768728733062744\n",
            "epoch: 10, iteratation 72, loss: 4.407228469848633\n",
            "epoch: 10, iteratation 73, loss: 4.172020435333252\n",
            "epoch: 10, iteratation 74, loss: 4.2006683349609375\n",
            "epoch: 10, iteratation 75, loss: 4.34867000579834\n",
            "epoch: 10, iteratation 76, loss: 4.250309944152832\n",
            "epoch: 10, iteratation 77, loss: 3.9761388301849365\n",
            "epoch: 10, iteratation 78, loss: 3.9185361862182617\n",
            "epoch: 10, iteratation 79, loss: 4.026246070861816\n",
            "epoch: 10, iteratation 80, loss: 4.815793514251709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer setup\n",
        "gpt_encoder = tiktoken.get_encoding('gpt2')\n",
        "tokens = gpt_encoder.encode(\"I am a doctor who\")\n",
        "tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)\n",
        "tokens = tokens.repeat(Batch_size, 1)\n",
        "\n",
        "# Sampling loop\n",
        "sample_rng = torch.Generator(device=get_device())\n",
        "sample_rng.manual_seed(42)\n",
        "\n",
        "while tokens.size(1) < Block_size:\n",
        "    with torch.no_grad():\n",
        "        tokens = tokens.to(get_device())\n",
        "        logits, _ = model(tokens)\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "        ix = torch.multinomial(topk_probs, 1, generator=sample_rng).to(get_device())\n",
        "        xcol = torch.gather(topk_indices, -1, ix)\n",
        "        tokens = torch.cat((tokens, xcol), dim=1)\n",
        "\n",
        "# Decode and print generated sequences\n",
        "for i in range(Batch_size):\n",
        "    generated_tokens = tokens[i, :].tolist()\n",
        "    generated_text = gpt_encoder.decode(generated_tokens)\n",
        "    print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJqnbddRwdCe",
        "outputId": "29ad7110-2505-4fbf-d78a-02a0639d03c8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am a doctor who'st thou\n",
            "And for my mother the first with him to take thee and let them yet; for my master:\n",
            "And so far good\n",
            "The soldier,\n",
            "A gentleman'suke's death?\n",
            "DUCHIO::\n",
            "And what's daughter is a\n",
            "PANCA:\n",
            "O:\n",
            "And give me for this Kate!\n",
            "For his country,\n",
            "And what thou,\n",
            "So I am to have been of a very to die.\n",
            "If the fire the maid be, let's no\n",
            "\n",
            "TRANCA:\n",
            "If then I am so I may be my lord,\n",
            "But now for ever had made\n",
            "And all well,\n",
            "A way,\n",
            "\n",
            "A:\n",
            "Of this place?\n",
            "\n",
            "\n",
            "But\n",
            "I will, here comes up;\n",
            "I will.\n",
            "When ever your lord, he is the better.\n",
            "You should ask-APTISABELLA: but I have so,\n",
            "\n",
            "As what does be anon with thee and he comes the devil.\n",
            "I cannot do.\n",
            "And else the world; if you,\n",
            "Your life;\n",
            "\n",
            "And yet so I am you, a devil will I will;\n",
            "\n",
            "The noble,\n",
            "\n",
            "\n",
            "As, it is,\n",
            "\n",
            "He hath I'll not not have made\n",
            "HORTENSIO:\n",
            "\n",
            "I am heard, the duke will see with my noble?\n",
            "We know you.\n",
            "I'll pardon not,\n",
            "But to you are a noble brother:\n",
            "ThWARDCDELLO:\n",
            "Hear, as for what does be so the little\n",
            "\n",
            "Than shall be not he's no more.\n",
            "GRUMARINA:\n",
            "\n",
            "I am\n",
            "You will\n",
            "And thou, as I would?\n",
            "And by her, and be; that shall be with her; not, but not with me as I will not more than a\n",
            "DUCHIO:\n",
            "That's to be well;\n",
            "DUCHIO:\n",
            "If thou wilt in a\n",
            "HORTENSIO:\n",
            "And that my lord, let me to prison,\n",
            "And see what the sun or thus.\n",
            "MENIVERS:\n",
            "What\n",
            "He, by their brother;\n",
            "As you shall,\n",
            "And think with you not; I have a\n",
            "Come.\n",
            "And know you, if you may have, by death;\n",
            "As, in her; if thy grace with thy lord.\n",
            "HORTENSIO:\n",
            "HORTENSIO:\n",
            "\n",
            "I am a doctor who as;\n",
            "Thy more than some\n",
            "I are no better\n",
            "Come,\n",
            "That have, my brother,\n",
            "HORTENSARINA:\n",
            "The people,\n",
            "To hear, is your good and yet;\n",
            "Of a devil of him:\n",
            "And to do, and bid'st in prison.\n",
            "What be a\n",
            "PETRUCHIO:\n",
            "And by the law well from me here.\n",
            "I will not.\n",
            "That, to\n",
            "\n",
            "What:\n",
            "DUCHIO:\n",
            "And, we had, and let it good Bianca's point.\n",
            "VINCENTIO: therefore are not not,\n",
            "HORTENSIO:\n",
            "Sir,\n",
            "First Citizen:\n",
            "That comes the king I find no gentleman is a wINCENTIO: now?\n",
            "PETA merchant, and bid me by this, as he is.\n",
            "And be patient;\n",
            "\n",
            "My tongue, I have\n",
            "To win a city\n",
            "I will have the matter.\n",
            "I must.\n",
            "Hade me to-school-upeling\n",
            "But in her own father are not have an great\n",
            "TROMPEHAM: 'gior LuENSIO:\n",
            "And I will be with her.\n",
            "O:\n",
            "Than I are I'll take you all the very thing.\n",
            "VINCENTARINA:\n",
            "Come, and a commonly, here!\n",
            "I'll not well.\n",
            "\n",
            "GRUMENTIO:\n",
            "Aea,\n",
            "ISABANARINA:\n",
            "Why, that;\n",
            "\n",
            "But I'll I know;\n",
            "I dare be well; on your father's to do not? I'll not be he shall be ready I may be to make;\n",
            "Which should be not not of thee, I am of him;\n",
            "And with them.\n",
            "I'll find thy husband have, a Capitol, then, let me;\n",
            "PETRUCHIO:\n",
            "\n",
            "A heart, as thou art not.\n",
            "PETRreat:\n",
            "And, I have so well.\n",
            "You must.\n",
            "\n",
            "I have made, the noble man is too to her.\n",
            "\n",
            "But that in me, for't; I may not made with his love;\n",
            "\n",
            "\n",
            "I do not he was.\n",
            "BIONINA:\n",
            "\n",
            "DUCHIO: I am:\n",
            "A:\n",
            "\n",
            "\n",
            "For a fool, I pray, the man;\n",
            "LUCENTOUCost of the jath with a fie, if not to them that\n",
            "I am a doctor who,\n",
            "When he!\n",
            "How should be in the world! what I'll go than\n",
            "\n",
            "Will be, she is a\n",
            "In the world, a world:\n",
            "And yet,\n",
            "Than are of that 'Sunday\n",
            "And,\n",
            "TRANCA:\n",
            "That I will swear at home as a\n",
            "He were in a city!\n",
            "TRANINA:\n",
            "IUS:\n",
            "A woman of the other\n",
            "To die in a soldier, and yet in the world's wife;\n",
            "In such I'll I have you here not\n",
            "And bring you will?\n",
            "That he has, as the word of the house of this will be not\n",
            "In a world's son's husband;\n",
            "TRANINA:\n",
            "And he's love, and by thy sword is but he will,\n",
            "And, so long for a doth I am made me with this is so I pray me to take you have you may be so still.\n",
            "\n",
            "BIONATHARINA:\n",
            "First Citizen:\n",
            "For her,\n",
            "\n",
            "I are he is\n",
            "But in all come;\n",
            "\n",
            "Away.\n",
            "The very word; but what comes;\n",
            "But of him well is well.\n",
            "Now,\n",
            "CORINA:\n",
            "\n",
            "And I may, you\n",
            "PETRUCHARINA:\n",
            "O:\n",
            "\n",
            "HORANCA: and to-morrow?\n",
            "As any,\n",
            "And well much! what is;\n",
            "I have he thinks not with his daughter's son.\n",
            "And,\n",
            "\n",
            "BAPRUCHIO: but as this is my lords!\n",
            "TRANIO:\n",
            "\n",
            "Come;\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "HORTENSIO:\n",
            "CALTis, go a shrewly to be more to the devil.\n",
            "As you do not here's great old\n",
            "DUCHIO:\n",
            "To your love,\n",
            "You be not a people's\n",
            "The first's no day!\n",
            "And I am thou lovath brought to the world and I am the better; so well of my brother's house; the dregcentIA:\n",
            "\n",
            "MENUCHARINA:\n",
            "But so:\n",
            "That's sake.\n",
            "To think this is not a devil!\n",
            "And have not of my head was here on our hands.\n",
            "And, thou I am to make\n",
            "I am:\n",
            "It hath not be that this one more and,\n",
            "And so. and in no more.\n",
            "You will I should be as you and now.\n",
            "We'll\n",
            "I am a doctor who's to\n",
            "As I honour.\n",
            "And why,\n",
            "C Citizen:\n",
            "Your mother is made with a way thou art, my lord? be but his master:\n",
            "O:\n",
            "B Citizen:\n",
            "What:\n",
            "I do.\n",
            "\n",
            "\n",
            "BAPRUCHIOER:\n",
            "IUS:\n",
            "Where's death and see,\n",
            "\n",
            "And so more, come,\n",
            "I am known,\n",
            "MENUCHIO: I will a devil, I are he should have:\n",
            "A man of the world?\n",
            "PETRUCHIO:\n",
            "If you, yet I wouldst not;\n",
            "Harp, but\n",
            "Well's house.\n",
            "PANCA:\n",
            "I'll bear of Pisa of one that of us in her, as it be well so in a\n",
            "To call a\n",
            "To take not.\n",
            "A: here, go.\n",
            "And have;\n",
            "If is your friends:\n",
            "\n",
            "HENreat it, and therefore,\n",
            "But, I will you will you say.\n",
            "My name? and leave not come at that may prove in me in such this:\n",
            "\n",
            "'TISABETH:\n",
            "But let, sir:\n",
            "He's the kind of some better; my husband,\n",
            "No, that'st as we are that is he was married! he is that you all\n",
            "\n",
            "CORoth she knows, for 'tis he were he stands with her,\n",
            "FirstUCHIO:\n",
            "I am so much than a city were to get it but the noble as to say she, I hope to get you to her more than more.\n",
            "ISABUCENTIO:\n",
            "And in this my very thing of this day to your master? I hear's sake!\n",
            "For this my masters!\n",
            "What my good!\n",
            "CAS:\n",
            "We say:\n",
            "\n",
            "The poor daughter and make thee in my lord will not.\n",
            "To be a little as by my wife!\n",
            "Sir, and that's death.\n",
            "Which, as time of her\n",
            "Well,\n",
            "He will.\n",
            "I not,\n",
            "Than had to be\n",
            "But, she is a\n",
            "\n",
            "PETRUTRANIO:\n",
            "My father\n",
            "O with no old, for he be to your poor for a\n",
            "PANIO: be here.\n",
            "Your lords,\n",
            "\n",
            "What, all\n",
            "\n",
            "PETRUCHIO:\n",
            "\n",
            "\n",
            "ISELO:\n",
            "MENURINA:\n",
            "KATHARINA:\n",
            "Now!\n",
            "If thou\n",
            "I am a doctor who?\n",
            "I should be with your husband,\n",
            "MENUCHIO:\n",
            "Orahrah, and the time, how I see it?\n",
            "For some more the good to die\n",
            "\n",
            "I'll be of me,\n",
            "I have at the most doth ever been well;\n",
            "\n",
            "It was but thus made it:\n",
            "Where are that I'll do find you at all as he is by\n",
            "PETRUCHIO:\n",
            "'en the first's sake:\n",
            "KATHARINA:\n",
            "\n",
            "To make him as she will\n",
            "PETRUCHIO:\n",
            "A house of these way to-morrow; my father's a dost thou'TO:\n",
            "BIONATHARINA:\n",
            "I have, I should be hanged not to you.\n",
            "\n",
            "\n",
            "SICOSY V Senator:\n",
            "You have to instruct you must find the cause as well?\n",
            "A:\n",
            "When her,\n",
            "The fool, you:\n",
            "When she must be now as I have a noble friend, I have a Capitol?\n",
            "\n",
            "What is he? how to give her.\n",
            "Away of my country's good:\n",
            "That I have\n",
            "Thou.\n",
            "We would not\n",
            "I have my lord?\n",
            "PETRUCHIO:\n",
            "\n",
            "My master's my life,\n",
            "\n",
            "That's pardon, and give yourself, at the\n",
            "\n",
            "To save your grace?\n",
            "How'st?\n",
            "Why's and take her: yet to prison.'\n",
            "FUCHIO:\n",
            "The fool and the city: your mind::\n",
            "\n",
            "Thou art with's,\n",
            "And you to be in a jest:\n",
            "\n",
            "For she.\n",
            "TRANCA:\n",
            "PETISTA:\n",
            "Why, by her.\n",
            "\n",
            "DUCHIO:\n",
            "TRANARINA:\n",
            "\n",
            "PANARINA:\n",
            "I'll be ready so.\n",
            "VINCENTIO:\n",
            "And make it; and with the time\n",
            "\n",
            "As time\n",
            "I'll call me with their own sister's head to\n",
            "\n",
            "KTA:\n",
            "\n",
            "KDEUDARINA:\n",
            "Where,\n",
            "\n",
            "We am come, Kate,\n",
            "SICOSPERO:\n",
            "\n",
            "If she knows to her love as he is a pleasant thousand.\n",
            "\n",
            "\n",
            "I'll-morrow:\n",
            "You must\n",
            "\n",
            "TRiond as time and tell me!\n",
            "For all for not be thus I may,\n",
            "KATHARINA:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "PETRUCH\n",
            "I am a doctor who's face and\n",
            "For the\n",
            "And still of't; if she will be a senate is married;\n",
            "As thou raudio's pleasure is to bed.\n",
            "\n",
            "\n",
            "GRUMIO:\n",
            "And,\n",
            "That was to bed,\n",
            "And all the devil.\n",
            "PANost:\n",
            "\n",
            "That is but of this.\n",
            "MENUCHIO: therefore he are all a rours in our brother and see\n",
            "I should not not the maid, I will be a soldier:\n",
            "As you\n",
            "And hepherdter,\n",
            "\n",
            "\n",
            "SICINA:\n",
            "And never is but a one of their world\n",
            "GREMIO:\n",
            "But the devil;\n",
            "O:\n",
            "VINCENTIO:\n",
            "TRANIO:\n",
            "That'st,\n",
            "'gain and a one than a woman'st for your words:\n",
            "That I will find to have a dothst:\n",
            "But in a\n",
            "\n",
            "\n",
            "And so, at Padua, I am his daughter:\n",
            "And then here.\n",
            "What, I'll buy this same.\n",
            "\n",
            "And if we were she is a noble Kate!\n",
            "I make:\n",
            "And a news.\n",
            "That shall be his duty hath been not a gods:\n",
            "TRANARINA:\n",
            "And well!\n",
            "As, for what's life, or that you;\n",
            "LUCIO:\n",
            "And, and I come.\n",
            "\n",
            "To win\n",
            "I wish me.\n",
            "He did, and else for one that a city man will hear,\n",
            "TRANUS:\n",
            "We must have for death are not a doth not.\n",
            "That is\n",
            "And be not speak for well:\n",
            "A:\n",
            "That I pray,\n",
            "We do with the\n",
            "And not content, that was to come he's no thing and see to your master:\n",
            "To have he'st,\n",
            "I would to him?\n",
            "LUCENTIO: I was so more and his wife.\n",
            "For they have, sir, to be that's not that of all so to be\n",
            "My master.\n",
            "\n",
            "\n",
            "GRUMIO:\n",
            "Sench?\n",
            "I have done:\n",
            "O:\n",
            "SICINA:\n",
            "KDECIUS:\n",
            "Andst for she must I will as my name?\n",
            "And know you are your state it.\n",
            "And,\n",
            "And make them that:\n",
            "He will be the moon.\n",
            "BAPTill we know,\n",
            "\n",
            "To be not:\n",
            "DUCHUMUMENTIO:\n",
            "\n",
            "I am a doctor who thinks them in look and bear it man and\n",
            "If, no noblely; of her times a way as he shall not all than a name of any thing, good\n",
            "PETRUCHIO:\n",
            "\n",
            "The good's wife were, if 'tis but\n",
            "A merchant is as,\n",
            "To be for my father,\n",
            "I'll see him as you the matter.\n",
            "BAPTISTA:\n",
            "CORINA:\n",
            "Thou dame, it.\n",
            "PETA:\n",
            "For thou hast thou hast.\n",
            "VINCENTIO:\n",
            "Well,\n",
            "Where is not have much with this:\n",
            "PETRUCHIO:\n",
            "I have made.\n",
            "And, for his honour,\n",
            "HENESC Messenger:\n",
            "I am a common!\n",
            "The father:\n",
            "What'st:\n",
            "That I have ready,\n",
            "My father\n",
            "PETRUCHIO:\n",
            "KATHARINA:\n",
            "LUCELLA:\n",
            "I am noORTENSIO:\n",
            "I will not a news and do thee, your daughter.\n",
            "FirstUCHUMUMUMIO:\n",
            "\n",
            "Hear so than a good and have you, SignANSON:\n",
            "GREMIBAPARINA:\n",
            "And call\n",
            "\n",
            "Which we would to-IONDEruchio is as thou bear your brother, for thyUCORUEENUTIO:\n",
            "BIONDELLO thou wert,\n",
            "O this is ready.\n",
            "But she hath; as that not.\n",
            "But let me to make thee in the word to-handest of some long's the people?\n",
            "A::\n",
            "This man is done, as I have to do not; my lord;\n",
            "\n",
            "He should do:\n",
            "A:\n",
            "A:\n",
            "And come to a gods to me by the world,\n",
            "For you may be well.\n",
            "\n",
            "SICINI may you say in me; to my lord, and my life.\n",
            "You are be no?\n",
            "O, so I are that's no one?\n",
            "And be a poor Kateest it:\n",
            "PETRUCHIO:\n",
            "BIONDEIO:\n",
            "I may be as you may take good;\n",
            "How then. I think the most old for a people.\n",
            "My life;\n",
            "A:\n",
            "For\n",
            "A:\n",
            "\n",
            "That were a world of your brother, my lord,\n",
            "F Senator:\n",
            "Well,\n",
            "DU Servensay,\n",
            "\n",
            "What was no time.\n",
            "CESCAL\n",
            "I am a doctor who's face\n",
            "I, if,\n",
            "And what's daves,\n",
            "Though she had thus like to my lord or she is and give her, as they do come of a husband, and make your friends of a world.\n",
            "\n",
            "\n",
            "\n",
            "And thou art with my good; and so of such.\n",
            "He'll-day to be more to-avery;\n",
            "You do well in a\n",
            "What,\n",
            "\n",
            "\n",
            "Than:\n",
            "What was a better.\n",
            "Well was now's go, and make her mother.\n",
            "O:\n",
            "\n",
            "BIONABlander,\n",
            "\n",
            "PETRUCHIO:\n",
            "Come,\n",
            "HORTENSIO:\n",
            "CALUS:\n",
            "\n",
            "He is my master'st to the fire was not to prison;\n",
            "\n",
            "\n",
            "Which else, for that's be as you say 'tis a people, to my lord, he is a fool:\n",
            "I do the world:\n",
            "And else to the one:\n",
            "\n",
            "TRYCUS:\n",
            "Thou lumbling; for a cause, as your own man! yet she must not,\n",
            "How comes so,\n",
            "And let him:\n",
            "And well.\n",
            "That's,\n",
            "\n",
            "And I saw the cause to the sun, that he will with thee are yet\n",
            "First Soldier:\n",
            "HORTENSIO:\n",
            "\n",
            "And, my wife, is,\n",
            "First Citizen:\n",
            "You can he's my lord?\n",
            "\n",
            "What's\n",
            "For all your house! my wife.\n",
            "DUUCHIO:\n",
            "PETRUCHUMENTIO: it is for I know'st.\n",
            "I mustst, so.\n",
            "KATHIO:\n",
            "Myself:.\n",
            "PETISTA'TISTA:\n",
            "\n",
            "\n",
            "'TISABELLA man and she's go of the cause to see, and not the kind!\n",
            "O:\n",
            "I have made\n",
            "Your heart.\n",
            "GRIO:\n",
            "If he is\n",
            "I never;\n",
            "How not as well from him and, and my brother?\n",
            "And make this;\n",
            "I will not.\n",
            "BIONDELLO:\n",
            "\n",
            "As your grace, is:\n",
            "I am not, for him.\n",
            "The\n",
            "BIONINA: I, and you know a gods, and, he's youngestIO:\n",
            "I have!\n",
            "And yet not that's to visit me to me,\n",
            "And thou art thou art, be more:\n",
            "That,\n",
            "The city, and make no more but here:\n"
          ]
        }
      ]
    }
  ]
}